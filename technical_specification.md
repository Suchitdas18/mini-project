# Continual Learning System for Adaptive Hate-Speech Detection
## Technical Specification & Implementation Blueprint

---

## Executive Summary

Online hate-speech detection systems face a fundamental challenge: linguistic toxicity evolves continuously through emerging slang, coded expressions, emoji combinations, and community-specific jargon. Traditional machine learning classifiers, trained once and deployed statically, degrade rapidly as adversarial actors adapt their language to circumvent detection. This specification defines a production-ready continual learning system that adapts to evolving hate-speech patterns while preserving detection capability for historical expressions.

The proposed system combines memory-based rehearsal, knowledge distillation, and selective regularization to enable incremental model updates without catastrophic forgetting. It ingests streaming abuse reports, prioritizes high-value examples through active learning, and triggers targeted model updates validated against temporal benchmarks. Beyond detection, the system provides actionable moderation recommendations, explainability summaries, and privacy-preserving rehearsal mechanisms.

This deliverable provides complete system architecture, API specifications, evaluation protocols, and an 8-week implementation roadmap suitable for immediate engineering execution. Expected outcomes include sustained detection performance (macro-F1 > 0.85) across evolving linguistic patterns, minimal backward forgetting (< 5% degradation on historical datasets), and sub-200ms inference latency at production scale.

---

## Final Problem Statement

Online hate-speech detection systems must operate in an adversarial, rapidly evolving linguistic environment where abusive actors continuously adopt new slang, emojis, abbreviations, and coded expressions to evade static classifiers. This project develops a continual learning framework for hate-speech detection that incrementally updates classification models over time while mitigating catastrophic forgetting through combined memory-based rehearsal and regularization techniques, enabling robust detection performance as language evolves while maintaining privacy, fairness, and interpretability constraints required for production deployment.

---

## System Objectives & Success Metrics

### Primary Objectives
- **Objective 1:** Maintain robust hate-speech detection across evolving linguistic patterns spanning multiple time periods
- **Objective 2:** Prevent catastrophic forgetting of historical hate-speech patterns when adapting to new expressions
- **Objective 3:** Minimize human annotation burden through intelligent active learning and weak supervision
- **Objective 4:** Provide actionable, explainable moderation recommendations with confidence calibration
- **Objective 5:** Ensure privacy-preserving rehearsal mechanisms that avoid storing raw sensitive content
- **Objective 6:** Deploy model updates with zero downtime and rollback capability

### Success Metrics

**Detection Performance:**
- **Macro-F1 Score:** â‰¥ 0.85 sustained across all evaluation periods
- **Per-Class Recall:** â‰¥ 0.80 for minority hate categories (prevents erasure of underrepresented abuse types)
- **Precision at High Confidence (â‰¥ 0.9):** â‰¥ 0.95 (minimizes false positives in auto-moderation)

**Continual Learning Metrics:**
- **Backward Transfer (BWT):** â‰¥ -0.05 (maximum 5% degradation on past tasks)
- **Forward Transfer (FWT):** â‰¥ 0.10 (positive transfer to new tasks)
- **Average Forgetting:** â‰¤ 0.03 across all historical evaluation sets
- **Adaptation Speed:** Achieve 80% of maximum performance within 500 labeled examples of new pattern

**Operational Metrics:**
- **Inference Latency:** p95 â‰¤ 200ms for single-text detection
- **Batch Throughput:** â‰¥ 1000 texts/second
- **Update Frequency:** Weekly automated retraining with drift-triggered emergency updates
- **Annotation Efficiency:** â‰¥ 70% reduction in required labels vs. random sampling (via active learning)

**Fairness & Safety:**
- **False Positive Disparity:** â‰¤ 0.15 across demographic groups (AAVE, regional dialects)
- **Appeal Overturn Rate:** â‰¤ 15% of flagged content overturned on human review
- **Explainability Coverage:** 100% of predictions accompanied by interpretable rationale

---

## High-Level Architecture Diagram (Textual Description)

### Component Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Ingestion â”‚â”€â”€â”
â”‚  Service        â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Content Routing Layer                    â”‚
â”‚  (Load balancer, rate limiter, deduplication)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼            â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Detector â”‚  â”‚ Detector â”‚  â”‚ Detector â”‚
â”‚   v1.3   â”‚  â”‚   v1.4   â”‚  â”‚  Canary  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚            â”‚            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Action Engine        â”‚
         â”‚  (moderation router)  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼            â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Auto-Action â”‚ â”‚  Human   â”‚ â”‚ Explainabilityâ”‚
â”‚  (hide/flag)â”‚ â”‚  Review  â”‚ â”‚   Generator   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  Queue   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Annotation Interface â”‚
         â”‚  (PII-redacted)       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Active Learning      â”‚
         â”‚  Selection Engine     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Continual Learning    â”‚
         â”‚ Training Pipeline     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼            â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model   â”‚  â”‚ Rehearsalâ”‚  â”‚ Validationâ”‚
â”‚  Store   â”‚  â”‚  Memory  â”‚  â”‚  Results â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Deployment (blue-green swap)
```

### Component Descriptions

1. **Data Ingestion Service**  
   Purpose: Receives real-time content streams from platforms (posts, comments, messages) with metadata.

2. **Content Routing Layer**  
   Purpose: Load balances requests, deduplicates repeated content, applies rate limiting, routes to appropriate detector version.

3. **Detector (versioned instances)**  
   Purpose: Executes hate-speech classification inference; supports A/B testing and canary deployments.

4. **Action Engine**  
   Purpose: Translates model outputs into platform-specific moderation actions (hide, soft-filter, escalate, notify).

5. **Auto-Action Module**  
   Purpose: Executes high-confidence automated moderation actions (e.g., hide content with confidence > 0.95).

6. **Human Review Queue**  
   Purpose: Routes uncertain predictions (0.5 < confidence < 0.95) to human moderators with context.

7. **Explainability Generator**  
   Purpose: Produces human-readable rationales for predictions using attention maps and lexical highlighting.

8. **Annotation Interface**  
   Purpose: Presents PII-redacted content to annotators; collects corrective labels and feedback.

9. **Active Learning Selection Engine**  
   Purpose: Prioritizes high-uncertainty, high-diversity, or drift-indicative examples for annotation.

10. **Continual Learning Training Pipeline**  
    Purpose: Orchestrates model updates combining new labels, rehearsal samples, and regularization.

11. **Model Store**  
    Purpose: Version-controlled repository of trained models with metadata (training date, performance metrics).

12. **Rehearsal Memory**  
    Purpose: Maintains privacy-preserving representative samples or synthetic prototypes from past distributions.

13. **Validation Results Store**  
    Purpose: Tracks temporal benchmark performance, drift metrics, and fairness audits across model versions.

14. **Monitoring & Alerting** (implicit)  
    Purpose: Tracks drift, performance degradation, latency, error rates; triggers automated retraining or rollback.

---

## Behavioral Requirements: Detection + Actions

The system must perform the following actions for each input text:

### 1. Flag & Classify
**Action:** Assign hate-speech label and confidence score.  
**Required API Fields:**
```json
{
  "label": "hate_speech" | "offensive" | "neutral",
  "confidence": 0.92,
  "subcategories": ["racial", "threatening"],
  "severity": "high" | "medium" | "low"
}
```

### 2. Propose Moderation Action
**Action:** Recommend platform-specific intervention based on confidence and severity.  
**Required API Fields:**
```json
{
  "suggested_action": "auto_hide" | "soft_filter" | "escalate_human" | "notify_user" | "no_action",
  "action_confidence": 0.89,
  "action_reasoning": "High-confidence racial slur detected with threatening context"
}
```

### 3. Redact/Obfuscate PII
**Action:** Remove personally identifiable information before presenting to annotators or storing in rehearsal.  
**Required API Fields:**
```json
{
  "redacted_text": "You're such a [REDACTED] from [LOCATION]",
  "redaction_map": {
    "type": "PII_MASK",
    "entities_removed": ["PERSON", "LOCATION"]
  }
}
```

### 4. Suggest Counter-Speech Templates
**Action:** Propose constructive responses to mitigate harm (optional feature for community moderation).  
**Required API Fields:**
```json
{
  "counter_speech_templates": [
    "This language violates our community standards. Please rephrase respectfully.",
    "We recognize diverse perspectives but require civil discourse."
  ]
}
```

### 5. Export Exemplar to Rehearsal Memory
**Action:** Identify high-value examples for continual learning rehearsal buffer.  
**Required API Fields:**
```json
{
  "is_exemplar": true,
  "exemplar_reason": "rare_subcategory" | "boundary_case" | "diverse_representation",
  "embedding": [0.23, -0.45, ...],  // optional: store embedding instead of raw text
  "timestamp": "2025-12-11T12:00:00Z"
}
```

### 6. Trigger Human Review or Automatic Retrain
**Action:** Escalate uncertain cases or accumulate drift signals.  
**Required API Fields:**
```json
{
  "requires_human_review": true,
  "review_priority": "high" | "medium" | "low",
  "drift_score": 0.34,
  "trigger_retrain": false  // true if drift exceeds threshold
}
```

### 7. Produce Explainability Summary
**Action:** Generate interpretable rationale for decision.  
**Required API Fields:**
```json
{
  "explanation": {
    "method": "attention_highlighting",
    "highlighted_tokens": ["slur", "threat"],
    "attention_weights": [0.82, 0.76],
    "rationale_text": "Flagged due to racial slur in combination with violent threat context."
  }
}
```

### Complete Detection Response Schema
```json
{
  "request_id": "uuid-12345",
  "text_hash": "sha256-abc...",
  "prediction": {
    "label": "hate_speech",
    "confidence": 0.92,
    "subcategories": ["racial", "threatening"],
    "severity": "high"
  },
  "moderation": {
    "suggested_action": "auto_hide",
    "action_confidence": 0.89,
    "action_reasoning": "High-confidence racial slur with threatening context"
  },
  "privacy": {
    "redacted_text": "You're such a [REDACTED]",
    "pii_removed": ["PERSON"]
  },
  "learning": {
    "is_exemplar": true,
    "exemplar_reason": "rare_subcategory",
    "requires_human_review": false,
    "drift_score": 0.12
  },
  "explanation": {
    "highlighted_tokens": ["slur", "threat"],
    "rationale_text": "Racial slur combined with violent threat context"
  },
  "metadata": {
    "model_version": "v1.4.2",
    "inference_time_ms": 145,
    "timestamp": "2025-12-11T12:00:00Z"
  }
}
```

---

## Data & Annotation Strategy

### Streaming Collection Pipeline

**Data Sources:**
- Platform-reported content (user flags, automated pre-screening)
- Random sampling of public posts (stratified by platform, community, language)
- Proactive monitoring of high-risk communities (gaming forums, political discussions)
- Synthetic adversarial examples generated via paraphrasing, emoji substitution, obfuscation

**Collection Frequency:** Continuous ingestion with hourly batching for processing

**Volume Targets:**
- 100K unlabeled texts/day baseline
- 1Kâ€“5K human-labeled texts/week (adaptive based on drift)
- 500â€“1K synthetic adversarial examples/week

### Weak Label Generation

**Automated Weak Labelers:**
1. **Lexicon-based heuristics:** Keyword matching against evolving slur dictionaries
2. **Cross-lingual transfer:** Translate to high-resource languages, detect, back-translate
3. **Ensemble consensus:** Combine predictions from multiple frozen baseline models
4. **Community voting:** Aggregate user reports (filtered for brigading)

**Weak Label Fusion:** Snorkel-style probabilistic modeling to denoise and weight weak sources

### Active Learning Policy

**Selection Criteria (prioritize samples with highest combined score):**
1. **Uncertainty:** Entropy > 0.7 or confidence in [0.45, 0.65] range
2. **Diversity:** Embedding distance from rehearsal memory centroids (HDBSCAN clustering)
3. **Drift signal:** Prediction disagreement between current and previous model versions
4. **Temporal novelty:** Texts containing n-grams absent from training data (TF-IDF novelty score)
5. **Fairness coverage:** Under-sampled demographic dialects (AAVE, non-native English)

**Query Strategy:** Batch-mode diversity sampling with submodular optimization (maximize coverage per annotation budget)

**Annotation Budget:** 5,000 labels/week; emergency drift events can trigger 10K burst budget

### Anonymization Rules

**PII Redaction (before storage or annotation):**
- Named entities (PERSON, ORGANIZATION, LOCATION) â†’ `[REDACTED]`
- Email addresses, phone numbers, usernames â†’ Hash-based pseudonyms
- Rare identifiers (unique URLs, alphanumeric IDs) â†’ Generic placeholders

**Privacy-Preserving Rehearsal:**
- **Option 1:** Store only text embeddings + label (no raw text)
- **Option 2:** Synthetic replay via paraphrasing model (T5-based text rewriting)
- **Option 3:** Prototype-based rehearsal (cluster centroids + nearest neighbors)

**Retention Policy:**
- Raw unlabeled data: 7 days then deleted
- Labeled data (PII-redacted): 2 years for model training; anonymized embeddings retained indefinitely
- Rehearsal memory: 10K samples rotated quarterly; privacy-preserved representations only

### Class Balance & Exemplar Selection for Rehearsal

**Class Balancing:**
- **Oversampling minority classes:** Racial slurs (15%), LGBTQ+ hate (10%), ableist language (8%)
- **Stratified sampling:** Maintain 30% hate / 20% offensive / 50% neutral distribution in rehearsal buffer

**Exemplar Selection Criteria:**
1. **Boundary cases:** Samples near decision boundary (0.4 < confidence < 0.6)
2. **Rare subcategories:** Under-represented hate types (caste-based, religious)
3. **Temporal diversity:** Equal representation across 6-month epochs
4. **Linguistic diversity:** Coverage of slang evolution (e.g., 2023 vs. 2024 expressions)
5. **Hard negatives:** Frequently misclassified neutral content (sarcasm, reclaimed slurs)

**Rehearsal Buffer Size:** 10,000 samples (updated via reservoir sampling weighted by diversity score)

---

## Continual Learning Design

### Combined Approach: Rehearsal + Distillation + Selective Regularization

**Architecture:** Transformer-based classifier (RoBERTa-base) with adapter layers for task-specific plasticity

**Strategy Components:**
1. **Memory-based rehearsal:** Maintain 10K representative samples; replay during training
2. **Knowledge distillation:** Penalize divergence from previous model's predictions on neutral content
3. **Elastic Weight Consolidation (EWC):** Regularize updates to parameters critical for past tasks
4. **Adapter-based learning:** Freeze base transformer; update lightweight adapter layers for new patterns

### Algorithm Outline (Pseudocode)

```python
# Continual Learning Update Loop
def continual_update_cycle():
    """
    Executed weekly or when drift_score > threshold
    """
    
    # 1. DRIFT DETECTION
    drift_score = compute_drift(current_model, validation_stream)
    if drift_score < DRIFT_THRESHOLD and time_since_update < 7_days:
        return  # Skip update
    
    # 2. SAMPLE SELECTION via Active Learning
    new_unlabeled_batch = fetch_recent_texts(limit=50_000)
    
    # Compute uncertainty, diversity, novelty scores
    scores = active_learning_scorer.score(
        texts=new_unlabeled_batch,
        current_model=current_model,
        rehearsal_memory=rehearsal_buffer
    )
    
    # Select top-K for annotation
    selected_indices = top_k_diverse(scores, k=5000)
    annotation_batch = new_unlabeled_batch[selected_indices]
    
    # 3. ANNOTATION (human or weak labels)
    labels = annotation_service.label(annotation_batch)  # Returns labels + confidence
    
    # Filter low-confidence weak labels
    high_quality_pairs = [(text, label) for text, label in zip(annotation_batch, labels) 
                          if label.confidence > 0.7]
    
    # 4. PREPARE TRAINING DATA
    # Combine new labels + rehearsal samples
    rehearsal_samples = rehearsal_buffer.sample(size=5000)
    training_data = high_quality_pairs + rehearsal_samples
    
    # 5. MODEL UPDATE with Regularization
    previous_model = copy.deepcopy(current_model)
    
    # Compute Fisher Information Matrix for EWC (importance of each parameter)
    fisher_info = compute_fisher_information(previous_model, rehearsal_samples)
    
    for epoch in range(NUM_EPOCHS):
        for batch in DataLoader(training_data):
            # Forward pass
            logits = current_model(batch.texts)
            
            # Loss components
            task_loss = cross_entropy(logits, batch.labels)
            
            # Knowledge distillation: match previous model's predictions on neutral examples
            if batch.contains_neutral():
                prev_logits = previous_model(batch.texts).detach()
                distillation_loss = kl_divergence(
                    softmax(logits / temperature),
                    softmax(prev_logits / temperature)
                )
            else:
                distillation_loss = 0
            
            # EWC regularization: penalize changes to important parameters
            ewc_loss = 0
            for name, param in current_model.named_parameters():
                if 'adapter' not in name:  # Only regularize base transformer
                    ewc_loss += (fisher_info[name] * (param - previous_model.state_dict()[name])**2).sum()
            
            # Combined loss
            total_loss = task_loss + Î»_distill * distillation_loss + Î»_ewc * ewc_loss
            
            # Backpropagation
            total_loss.backward()
            optimizer.step()
    
    # 6. UPDATE REHEARSAL MEMORY
    # Add new exemplars using reservoir sampling
    new_exemplars = select_exemplars(
        high_quality_pairs, 
        criteria=['boundary_case', 'rare_class', 'temporal_diversity']
    )
    rehearsal_buffer.update(new_exemplars, strategy='reservoir_sampling')
    
    # 7. VALIDATION
    validation_results = evaluate_temporal_benchmarks(
        model=current_model,
        benchmarks=[task_t0, task_t1, task_t2, ..., task_current]
    )
    
    metrics = {
        'current_f1': validation_results.current.macro_f1,
        'backward_transfer': compute_BWT(validation_results),
        'average_forgetting': compute_forgetting(validation_results),
        'fairness_disparity': audit_fairness(current_model, demographic_testsets)
    }
    
    # 8. DEPLOYMENT DECISION
    if metrics['current_f1'] > 0.85 and metrics['backward_transfer'] > -0.05:
        # Deploy via blue-green swap
        deploy_model(current_model, version=f"v{timestamp}")
        log_metrics(metrics)
    else:
        # Rollback or retune hyperparameters
        alert_team("Model update failed validation", metrics)
        rollback_to_previous_version()
    
    return metrics


# Helper Functions
def compute_drift(model, stream):
    """
    Detect distribution shift via prediction disagreement and embedding distance
    """
    predictions = []
    embedding_shifts = []
    
    for batch in stream:
        current_pred = model.predict(batch)
        baseline_pred = baseline_model.predict(batch)  # Frozen snapshot model
        
        disagreement = (current_pred != baseline_pred).mean()
        predictions.append(disagreement)
        
        # Embedding drift
        current_emb = model.encode(batch)
        baseline_emb = baseline_model.encode(batch)
        emb_distance = cosine_distance(current_emb.mean(0), baseline_emb.mean(0))
        embedding_shifts.append(emb_distance)
    
    drift_score = 0.5 * np.mean(predictions) + 0.5 * np.mean(embedding_shifts)
    return drift_score


def select_exemplars(labeled_pairs, criteria):
    """
    Select diverse, informative samples for rehearsal memory
    """
    scores = defaultdict(list)
    
    for text, label in labeled_pairs:
        if 'boundary_case' in criteria:
            uncertainty = entropy(model.predict_proba(text))
            scores['boundary'].append(uncertainty)
        
        if 'rare_class' in criteria:
            class_frequency = label_distribution[label]
            scores['rare'].append(1.0 / class_frequency)
        
        if 'temporal_diversity' in criteria:
            novelty = tfidf_novelty_score(text, historical_corpus)
            scores['novelty'].append(novelty)
    
    # Combine scores and select top-K
    combined_score = np.mean([scores[k] for k in scores.keys()], axis=0)
    top_indices = np.argsort(combined_score)[-1000:]  # Select top 1000
    
    return [labeled_pairs[i] for i in top_indices]
```

### Hyperparameters

```python
DRIFT_THRESHOLD = 0.25
NUM_EPOCHS = 3
LEARNING_RATE = 2e-5
BATCH_SIZE = 32
Î»_distill = 0.5  # Distillation loss weight
Î»_ewc = 0.3      # EWC regularization weight
TEMPERATURE = 2.0  # Distillation temperature
REHEARSAL_BUFFER_SIZE = 10_000
ANNOTATION_BUDGET_WEEKLY = 5_000
```

---

## API Specification (Sample Endpoints)

### 1. `/detect` â€” Single Text Detection

**Request:**
```json
POST /api/v1/detect
Content-Type: application/json

{
  "text": "You're such a worthless piece of trash ðŸ¤¡",
  "context": {
    "platform": "reddit",
    "community": "r/gaming",
    "user_history_flags": 2
  },
  "options": {
    "include_explanation": true,
    "redact_pii": true,
    "suggest_action": true
  }
}
```

**Response:**
```json
{
  "request_id": "det-f8a3c9b2",
  "status": "success",
  "prediction": {
    "label": "hate_speech",
    "confidence": 0.87,
    "subcategories": ["dehumanizing", "personal_attack"],
    "severity": "high"
  },
  "moderation": {
    "suggested_action": "escalate_human",
    "action_confidence": 0.72,
    "action_reasoning": "High severity but moderate confidence; recommend human review"
  },
  "privacy": {
    "redacted_text": "You're such a worthless piece of trash ðŸ¤¡",
    "pii_removed": []
  },
  "explanation": {
    "method": "attention_highlighting",
    "highlighted_tokens": ["worthless", "trash", "ðŸ¤¡"],
    "attention_weights": [0.82, 0.79, 0.65],
    "rationale_text": "Dehumanizing language combined with contemptuous emoji usage"
  },
  "metadata": {
    "model_version": "v1.4.2",
    "inference_time_ms": 142,
    "timestamp": "2025-12-11T12:00:00Z"
  }
}
```

---

### 2. `/detect_batch` â€” Batch Detection

**Request:**
```json
POST /api/v1/detect_batch
Content-Type: application/json

{
  "texts": [
    "Great job on the project!",
    "KYS you absolute waste of oxygen",
    "This game is mid tbh"
  ],
  "options": {
    "include_explanation": false,
    "return_only_flagged": true
  }
}
```

**Response:**
```json
{
  "request_id": "batch-9d2e4f1a",
  "status": "success",
  "results": [
    {
      "index": 1,
      "text_hash": "sha256-xyz...",
      "prediction": {
        "label": "hate_speech",
        "confidence": 0.94,
        "subcategories": ["suicide_encouragement", "dehumanizing"],
        "severity": "critical"
      },
      "moderation": {
        "suggested_action": "auto_hide",
        "action_confidence": 0.91
      }
    }
  ],
  "summary": {
    "total_processed": 3,
    "flagged_count": 1,
    "processing_time_ms": 287
  }
}
```

---

### 3. `/action_suggest` â€” Moderation Action Recommendation

**Request:**
```json
POST /api/v1/action_suggest
Content-Type: application/json

{
  "prediction": {
    "label": "offensive",
    "confidence": 0.76,
    "severity": "medium"
  },
  "context": {
    "user_violation_history": 1,
    "community_sensitivity": "high",
    "content_type": "public_comment"
  }
}
```

**Response:**
```json
{
  "request_id": "act-2b7c3e8d",
  "recommended_actions": [
    {
      "action": "soft_filter",
      "confidence": 0.82,
      "description": "Hide content behind 'potentially offensive' warning",
      "reversible": true
    },
    {
      "action": "notify_user",
      "confidence": 0.68,
      "description": "Send warning notification to author",
      "reversible": true
    }
  ],
  "escalation_threshold": {
    "next_violation_action": "temporary_ban",
    "threshold_count": 3
  }
}
```

---

### 4. `/export_exemplar` â€” Add to Rehearsal Memory

**Request:**
```json
POST /api/v1/export_exemplar
Content-Type: application/json

{
  "text": "This new coded slang expression",
  "label": "hate_speech",
  "confidence": 0.91,
  "metadata": {
    "source": "human_review",
    "subcategory": "emerging_slang",
    "timestamp": "2025-12-11T12:00:00Z"
  },
  "reason": "rare_subcategory"
}
```

**Response:**
```json
{
  "request_id": "exp-4f9a2c1b",
  "status": "success",
  "exemplar_id": "exm-88a3bc72",
  "added_to_buffer": true,
  "buffer_stats": {
    "current_size": 9847,
    "capacity": 10000,
    "evicted_count": 0
  }
}
```

---

### 5. `/trigger_update` â€” Initiate Continual Learning Update

**Request:**
```json
POST /api/v1/trigger_update
Content-Type: application/json

{
  "trigger_reason": "drift_detected",
  "drift_score": 0.34,
  "labeled_batch": [
    {
      "text": "New emerging hate pattern",
      "label": "hate_speech",
      "confidence": 0.95,
      "source": "human_annotator"
    }
  ],
  "options": {
    "emergency_update": false,
    "skip_validation": false
  }
}
```

**Response:**
```json
{
  "request_id": "upd-7e2d9f3a",
  "status": "accepted",
  "update_job_id": "job-continual-20251211-001",
  "estimated_completion": "2025-12-11T15:00:00Z",
  "pipeline_stages": [
    {
      "stage": "data_preparation",
      "status": "queued"
    },
    {
      "stage": "model_training",
      "status": "pending"
    },
    {
      "stage": "validation",
      "status": "pending"
    },
    {
      "stage": "deployment",
      "status": "pending"
    }
  ],
  "webhook_url": "https://api.example.com/webhooks/update_complete"
}
```

**Status Check Endpoint:**
```json
GET /api/v1/update_status/job-continual-20251211-001

Response:
{
  "job_id": "job-continual-20251211-001",
  "status": "training",
  "progress": 0.67,
  "current_stage": "model_training",
  "metrics": {
    "samples_processed": 8340,
    "current_epoch": 2,
    "training_loss": 0.23
  },
  "eta_seconds": 1200
}
```

---

## Evaluation Protocol & Benchmarks

### Temporal Evaluation Setup

**Benchmark Structure:**
Create time-stamped evaluation sets representing linguistic evolution:

- **T0 (Baseline):** Historical hate-speech (2020â€“2021) â€” established slurs, explicit threats
- **T1 (Slang emergence):** 2022 data â€” new slang like "mid", "ratio", coded emojis
- **T2 (Abbreviation shift):** 2023 data â€” abbreviated hate ("KYS", "unalive")
- **T3 (Current):** 2024â€“2025 data â€” latest expressions, multi-modal hate (emoji combinations)
- **T_future (Adversarial):** Synthetic future patterns via paraphrasing and obfuscation

**Held-Out Tasks:**
- **Cross-community transfer:** Train on Reddit, test on Twitter/TikTok
- **Cross-lingual transfer:** Train on English, test on code-switched Hinglish/Spanglish
- **Cross-demographic:** Test on AAVE, regional dialects, non-native English

### Metrics to Compute Per Time-Step

**Per Task (T0, T1, T2, ...):**

```python
metrics_per_task = {
    "macro_f1": macro_averaged_f1_score,
    "per_class_recall": {
        "hate_speech": recall_hate,
        "offensive": recall_offensive,
        "neutral": recall_neutral
    },
    "precision_at_high_conf": precision_where_confidence_gt_0.9,
    "auc_roc": area_under_roc_curve,
    "calibration_error": expected_calibration_error
}
```

**Continual Learning Metrics:**

```python
# After training on tasks T0 â†’ T1 â†’ T2 â†’ ... â†’ Tk
backward_transfer = mean([
    (acc_after_task_Tk[Ti] - acc_immediately_after_Ti[Ti]) 
    for Ti in [T0, T1, ..., T(k-1)]
])

forward_transfer = mean([
    (acc_with_transfer[Ti] - acc_from_scratch[Ti])
    for Ti in [T1, T2, ..., Tk]
])

average_forgetting = mean([
    max([acc_after_Tj[Ti] for Tj > Ti]) - acc_after_Tk[Ti]
    for Ti in [T0, T1, ..., T(k-1)]
])
```

**Fairness Metrics (per demographic group G):**

```python
fairness_metrics = {
    "false_positive_rate": FPR_per_group[G],
    "false_negative_rate": FNR_per_group[G],
    "demographic_parity": abs(P(Å·=hate | G=g1) - P(Å·=hate | G=g2)),
    "equalized_odds": max_over_groups(FPR_difference, FNR_difference)
}
```

### Baseline Comparisons

**Run evaluations against:**

1. **Static Baseline:** RoBERTa trained once on T0, never updated
2. **Naive Fine-tuning:** Sequential fine-tuning on each task without rehearsal (catastrophic forgetting)
3. **Full Retraining:** Train from scratch on all data each time (computationally expensive upper bound)
4. **EWC-only:** Elastic Weight Consolidation without rehearsal
5. **Rehearsal-only:** Memory replay without regularization
6. **Distillation-only:** Knowledge distillation without rehearsal
7. **Commercial APIs:** OpenAI Moderation API, Perspective API (for reference)

**Evaluation Frequency:**
- Weekly automated benchmarking after each update cycle
- Monthly comprehensive audits including fairness and adversarial robustness

---

## Safety, Bias & Ethics Checklist

### Annotator Diversity & Training

- [ ] **Diverse annotator pool:** Recruit annotators spanning race, gender, geography, age (minimum 30% representation from marginalized groups)
- [ ] **Annotator training:** Provide 8-hour hate-speech taxonomy training with examples of coded hate, reclaimed slurs, cultural context
- [ ] **Living wage compensation:** Pay annotators â‰¥ $15/hour with mental health support resources
- [ ] **Rotation policy:** Limit annotators to 4 hours/day on toxic content; provide 1-week breaks every month
- [ ] **Disagreement resolution:** Multi-annotator consensus (â‰¥2 reviewers per sample); escalate to expert panel for ties

### Auditing & Monitoring

- [ ] **Automated bias scans:** Weekly audits of false positive/negative rates across demographic groups (AAVE, LGBTQ+ terminology, religious language)
- [ ] **Red-teaming:** Monthly adversarial testing with linguists to identify evasion tactics
- [ ] **Transparency reports:** Quarterly public reports on model performance, error analysis, fairness metrics
- [ ] **Version control:** Immutable audit logs of all model versions, training data, and decisions
- [ ] **External audits:** Annual third-party fairness audit by independent organization

### False Positive Mitigation

- [ ] **Reclaimed slur detection:** Context-aware handling of in-group reclamation (e.g., LGBTQ+ community usage)
- [ ] **Cultural context models:** Specialized detectors for African-American Vernacular English (AAVE), regional dialects
- [ ] **Confidence thresholds:** Auto-moderation only for confidence > 0.90; human review for 0.50â€“0.90 range
- [ ] **Counter-examples database:** Maintain repository of false positives to continually refine model
- [ ] **User education:** When flagging borderline cases, provide explanations of community guidelines

### Appeal Workflow

- [ ] **User appeal mechanism:** One-click appeal for all automated moderation actions
- [ ] **Human review SLA:** Appeals reviewed by human moderator within 24 hours
- [ ] **Overturn feedback loop:** All overturned decisions fed back to model as corrective labels
- [ ] **Transparency:** Users receive explanation of why content was flagged and what guideline was violated
- [ ] **Graduated penalties:** First offense = warning; repeated violations = escalating actions

### Logging & Retention

- [ ] **Prediction logging:** Store all predictions with confidence, model version, timestamp for auditing
- [ ] **PII protection:** Never log raw text with PII; redact before storage
- [ ] **Retention limits:** Delete raw unlabeled data after 7 days; anonymized labeled data retained 2 years
- [ ] **Right to deletion:** Users can request deletion of their content from training data (GDPR compliance)
- [ ] **Access controls:** Annotators see only PII-redacted content; raw data accessible only to vetted engineers

### Addressing Systemic Bias

- [ ] **Pre-deployment bias audit:** Test on datasets representing marginalized communities before launch
- [ ] **Feedback from affected communities:** Quarterly consultations with advocacy groups (Anti-Defamation League, GLAAD, NAACP)
- [ ] **Differential privacy:** Apply DP-SGD during training to prevent memorization of individual examples
- [ ] **Bias bounty program:** Reward external researchers who identify fairness issues ($500â€“$5000 per validated report)

---

## Minimal Implementation Plan (Milestones & Artifacts)

### Week 1: Foundation & Data Pipeline
**Deliverables:**
- Data ingestion service operational (connects to platform APIs, stores to cloud buckets)
- PII redaction pipeline functional (NER-based entity removal, hash-based pseudonymization)
- Initial labeled dataset acquired (10K samples across hate/offensive/neutral; commercially licensed or crowdsourced)
- Benchmark evaluation sets curated (T0 baseline + T1 emerging slang; 2K samples each)

**Demoable:** Streaming data collector that redacts PII and stores anonymized samples

---

### Week 2: Baseline Model Training
**Deliverables:**
- Static baseline classifier trained (RoBERTa-base fine-tuned on Week 1 dataset)
- Inference API deployed (`/detect` endpoint operational with <200ms latency)
- Basic explainability integrated (attention-based token highlighting)
- Initial benchmark evaluation completed (document macro-F1, per-class recall on T0/T1)

**Demoable:** Working API that accepts text, returns hate-speech prediction with highlighted tokens

---

### Week 3: Active Learning & Annotation Interface
**Deliverables:**
- Active learning selection engine operational (uncertainty + diversity scoring)
- Human annotation interface deployed (PII-redacted, supports multi-label taxonomy)
- First batch of 1K actively-selected samples annotated
- Weak labeling pipeline functional (lexicon + ensemble-based heuristics)

**Demoable:** Annotation interface showing high-uncertainty examples; active learning reduces labeling by 60% vs random

---

### Week 4: Rehearsal Memory & Continual Learning Pipeline (v1)
**Deliverables:**
- Rehearsal buffer implemented (reservoir sampling, stores 5K embeddings + labels)
- Continual learning training script operational (rehearsal + basic EWC)
- First model update executed (train on Week 3 annotations + rehearsal samples)
- Backward transfer evaluation (measure forgetting on T0 after training on T1)

**Demoable:** Model updated on new slang data; demonstrates <5% accuracy drop on historical hate-speech

---

### Week 5: Moderation Action Engine & Deployment Infrastructure
**Deliverables:**
- Action suggestion API operational (`/action_suggest` returns hide/filter/escalate recommendations)
- Blue-green deployment pipeline functional (zero-downtime model swaps)
- A/B testing framework deployed (10% traffic to new model variant)
- Monitoring dashboards operational (latency, throughput, drift score, error rates)

**Demoable:** New model deployed via blue-green swap; A/B test shows performance comparison

---

### Week 6: Drift Detection & Automated Retraining
**Deliverables:**
- Drift detection system operational (prediction disagreement + embedding shift metrics)
- Automated update trigger functional (initiates retraining when drift > 0.25)
- Knowledge distillation integrated (preserves neutral-class predictions from previous model)
- Full continual learning loop executed end-to-end (drift detected â†’ annotation â†’ update â†’ validation â†’ deploy)

**Demoable:** Inject synthetic drift (new adversarial samples); system automatically detects, retrains, validates, deploys

---

### Week 7: Fairness Auditing & Bias Mitigation
**Deliverables:**
- Demographic testsets acquired (AAVE, LGBTQ+ terminology, religious language; 1K samples each)
- Fairness audit completed (false positive/negative disparity across groups documented)
- Bias mitigation applied (re-weighted loss, counterfactual data augmentation)
- Appeal workflow implemented (users can flag false positives; feeds to annotation queue)

**Demoable:** Fairness dashboard showing FPR disparity reduced from 0.32 to 0.12 across demographic groups

---

### Week 8: End-to-End Integration & Stress Testing
**Deliverables:**
- Full system integration tested (ingestion â†’ detection â†’ action â†’ review â†’ update â†’ deploy)
- Load testing completed (validates 1000 texts/second throughput, <200ms p95 latency)
- Documentation finalized (API docs, deployment runbooks, troubleshooting guides)
- Final benchmark evaluation (all temporal tasks T0â€“T3; BWT, FWT, forgetting metrics)
- Executive summary report delivered (performance vs. objectives, cost analysis, next-phase roadmap)

**Demoable:** Live system handling production-scale traffic; complete continual learning cycle from drift detection to model update in <2 hours

---

### Minimal Runnable Prototype Description

**What is demoable at Week 4 (MVP):**
- User submits hate-speech text via API
- System returns classification (hate/offensive/neutral) with confidence
- Highlighted tokens show which words triggered the decision
- Low-confidence predictions route to annotation interface
- Human annotator labels examples (PII-redacted view)
- System adds annotations to rehearsal buffer
- Weekly update cycle trains new model on annotations + rehearsed samples
- Validation confirms new model improves on new patterns without forgetting old ones
- Blue-green deployment swaps to new model with zero downtime

**What is demoable at Week 8 (Full System):**
All of the above, plus:
- Automated drift detection triggers emergency retraining
- Moderation action engine suggests platform-specific interventions
- Fairness audits run automatically; alerts if demographic disparity exceeds threshold
- Users can appeal false positives; appeals feed back to training data
- A/B testing framework evaluates new models on 10% traffic before full deployment
- Monitoring dashboards track performance, drift, latency, and fairness metrics in real-time

---

## One-Paragraph "How It Acts" Example

A user posts the comment *"unalive yourself you worthless NPC ðŸ’€"* on a gaming forum. The system detects the text, runs inference using the latest continual learning model (v1.4.2), and returns a prediction of "hate_speech" with 91% confidence, subcategorized as "suicide_encouragement" and "dehumanizing." The explainability module highlights ["unalive", "worthless", "NPC", "ðŸ’€"] as key tokens contributing to the decision. The action engine recommends "auto_hide" due to high confidence, immediately removing the comment from public view and notifying the moderation team. The system flags this example as an "emerging-slang" exemplar (due to the coded term "unalive") and adds its privacy-preserving embedding to the rehearsal buffer. Concurrently, the drift monitor detects a 12% increase in similar coded suicide-encouragement language over the past week. When the cumulative drift score crosses the 0.25 threshold on Friday, the system automatically triggers the continual learning pipeline: it selects 5,000 high-uncertainty recent samples for annotation, combines them with 5,000 rehearsed historical examples, trains an updated model using distillation and EWC regularization, validates performance across all temporal benchmarks (confirming macro-F1 of 0.87 with backward transfer of -0.03), and deploys the new model via blue-green swap Sunday morningâ€”ensuring the classifier evolves to detect "unalive" patterns while retaining detection capability for historical explicit threats like "kill yourself."

---

**END OF SPECIFICATION**

---

### Document Control

**Version:** 1.0  
**Date:** 2025-12-11  
**Status:** Production-Ready Specification  
**Target Audience:** Engineering teams, project sponsors, grant reviewers  
**Classification:** Internal / Confidential  

**Revision History:**
- v1.0 (2025-12-11): Initial production specification

**Approval Required From:**
- [ ] ML/AI Lead
- [ ] Platform Engineering Lead
- [ ] Trust & Safety Lead
- [ ] Legal/Privacy Counsel
- [ ] Product Manager

**Next Steps:**
1. Stakeholder review and approval (1 week)
2. Resource allocation and team assignment (Week 1)
3. Kickoff Sprint 0 (infrastructure provisioning, dataset licensing)
4. Begin Week 1 implementation per milestones above
